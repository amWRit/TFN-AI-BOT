#!/usr/bin/env python3
"""
ğŸš€ TFN RAG PIPELINE ORCHESTRATOR
Smartly runs scraper â†’ preprocessors â†’ vector store with caching

Usage:
    python scripts/pipeline.py --rebuild
    python scripts/pipeline.py --preprocess-only
"""

import os
import sys
import subprocess
import argparse
from pathlib import Path


def clean_outputs(mode=None, rebuild=False):
    """Delete only relevant output JSONs based on mode."""
    files = {
        'scraped': "public/json/scraped_data.json",
        'structured': "public/json/structured_data.json",
        'unstructured': "public/json/unstructured_chunks.json",
        'combined': "public/json/combined_data.json"
    }
    to_delete = []
    
    if rebuild:
        to_delete = list(files.values())
    elif mode == 'scrape_only':
        to_delete = [files['scraped']]
    elif mode == 'preprocess_only':
        to_delete = [files['structured'], files['unstructured']]
    
    for json_path in to_delete:
        if os.path.exists(json_path):
            try:
                os.remove(json_path)
                print(f"ğŸ§¹ DELETED: {json_path}")
            except Exception as e:
                print(f"âš ï¸  Could not delete {json_path}: {e}")
        else:
            print(f"â„¹ï¸  {json_path} not found (already clean)")


def run_scraper():
    """ğŸŒ Run web scraper tool."""
    print("ğŸŒ [*] Running scraper...")
    result = subprocess.run([
        sys.executable, "-m", "scripts.scraper",  # ğŸ‘ˆ UPDATED!
        "--scrape-all"
    ], capture_output=False, text=True)
    
    if result.returncode != 0:
        print("âŒ Scraper failed!")
        return False
    print("âœ… Scraper complete! â†’ scraped_data.json")
    return True


def run_preprocessor():
    """ğŸ“„ Run structured + unstructured preprocessors."""
    print("ğŸ“„ [*] Running preprocessors...")
    
    # Structured PDFs â†’ structured_data.json
    print("    ğŸ“‹ Structured preprocessor...")
    cmd_structured = [sys.executable, "-m", "scripts.structured_preprocessor"]
    result_structured = subprocess.run(cmd_structured, capture_output=False, text=True)
    if result_structured.returncode != 0:
        print("âŒ Structured preprocessor failed!")
        return False
    print("    âœ… Structured complete!")
    
    # Unstructured PDFs â†’ unstructured_chunks.json
    print("    ğŸ“„ Unstructured preprocessor...")
    cmd_unstructured = [sys.executable, "-m", "scripts.unstructured_preprocessor"]
    result_unstructured = subprocess.run(cmd_unstructured, capture_output=False, text=True)
    if result_unstructured.returncode != 0:
        print("âŒ Unstructured preprocessor failed!")
        return False
    print("    âœ… Unstructured complete!")
    
    return True


def run_indexbuilder(rebuild=False):
    """ğŸ”— Run index builder tool."""
    print("ğŸ”— [*] Running index builder...")
    cmd = [sys.executable, "-m", "scripts.indexbuilder"]  # updated here
    if rebuild:
        cmd.append("--rebuild")
    result = subprocess.run(cmd, capture_output=False, text=True)
    
    if result.returncode != 0:
        print("âŒ Index builder failed!")
        return False
    print("âœ… Index ready! â†’ public/vector-store/")
    return True

def check_prerequisites():
    """ğŸ“ Ensure all directories exist."""
    required_dirs = [
        "data/unstructured",
        "data/structured",
        "public/json",
        "public/vector-store"
    ]
    
    missing = []
    for dir_path in required_dirs:
        if not os.path.exists(dir_path):
            missing.append(dir_path)
            os.makedirs(dir_path, exist_ok=True)
            print(f"ğŸ“ Created: {dir_path}")
    
    if missing:
        print(f"ğŸ“ Created {len(missing)} directories")
    
    # PDF status
    structured_pdfs = list(Path("data/structured").glob("*.pdf"))
    unstructured_pdfs = list(Path("data/unstructured").glob("*.pdf"))
    
    print(f"ğŸ“ {len(structured_pdfs)} structured PDFs")
    print(f"ğŸ“ {len(unstructured_pdfs)} unstructured PDFs")
    
    return True


def main():
    parser = argparse.ArgumentParser(description="ğŸš€ TFN RAG Pipeline")
    parser.add_argument("--rebuild", action="store_true", 
                       help="Force rebuild vector store (ignores cache)")
    parser.add_argument("--scrape-only", action="store_true", 
                       help="ğŸŒ Scraper only")
    parser.add_argument("--preprocess-only", action="store_true", 
                       help="ğŸ“„ Preprocessors only")
    args = parser.parse_args()
    
    print("="*80)
    print("ğŸš€ TFN RAG PIPELINE")
    print("="*80)
    
    # cleaning
    if args.scrape_only:
        mode = 'scrape_only'
    elif args.preprocess_only:
        mode = 'preprocess_only'
    else:
        mode = 'full'  # Default full pipeline

    clean_outputs(mode=mode, rebuild=args.rebuild)
    
    if not check_prerequisites():
        return 1
    
    success = True
    
    if args.scrape_only:
        success = run_scraper()
    elif args.preprocess_only:
        success = run_preprocessor()
    else:
        # Full RAG pipeline
        success = (run_scraper() and 
                  run_preprocessor() and 
                  run_indexbuilder(args.rebuild))
    
    if success:
        print("\nğŸ‰ PIPELINE COMPLETE!")
        print("ğŸ“Š Data ready in public/json/ & public/vector-store/")
        print("="*80)
        return 0
    else:
        print("\nâŒ PIPELINE FAILED!")
        return 1


if __name__ == "__main__":
    exit(main())
